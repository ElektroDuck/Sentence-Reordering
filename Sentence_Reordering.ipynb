{"cells":[{"cell_type":"markdown","metadata":{"id":"ElNaMbLnRdHR"},"source":["# Sentence Reconstruction"]},{"cell_type":"markdown","metadata":{"id":"oXr4iGUGRms8"},"source":["The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n","\n","The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n","\n","\n","CONSTRAINTS:\n","* No pretrained model can be used.\n","* The neural network models should have less the 20M parameters.\n","* No postprocessing should be done (e.g. no beamsearch)\n","* You cannot use additional training data.\n","\n","\n","BONUS PARAMETERS:\n","\n","A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-11T17:13:14.859887Z","iopub.status.busy":"2024-06-11T17:13:14.859506Z","iopub.status.idle":"2024-06-11T17:13:41.563083Z","shell.execute_reply":"2024-06-11T17:13:41.561865Z","shell.execute_reply.started":"2024-06-11T17:13:14.859855Z"},"id":"YnGsq3WSamPP","outputId":"2b023460-e954-44de-a6c4-e8aa19754671","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\n","Requirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.3.3)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\n","Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.11.0)\n","Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.2.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.9.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"]}],"source":["!pip install datasets\n","!pip install --upgrade keras"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:13:41.565507Z","iopub.status.busy":"2024-06-11T17:13:41.565175Z","iopub.status.idle":"2024-06-11T17:13:41.573583Z","shell.execute_reply":"2024-06-11T17:13:41.572687Z","shell.execute_reply.started":"2024-06-11T17:13:41.565478Z"},"id":"_WjtqA8TrHcS","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","from keras import ops\n","from keras import layers\n","from keras.layers import Embedding\n","\n","from datasets import load_dataset\n","import string\n","import re\n","\n","import numpy as np\n","import math"]},{"cell_type":"markdown","metadata":{"id":"iQ8k-L-WUK7l"},"source":["## Data preprocessing\n","\n","1) Download the data\n","\n","2) Create the tokenizer and detokenizer\n","\n","3) Remove the sentences containing uknown tokens\n","\n","4) Create a generator to feed the data to the model while trainig, validating and testing"]},{"cell_type":"markdown","metadata":{"id":"807Wk-ir_bDU"},"source":["### Download the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:13:41.575092Z","iopub.status.busy":"2024-06-11T17:13:41.574725Z","iopub.status.idle":"2024-06-11T17:13:41.609321Z","shell.execute_reply":"2024-06-11T17:13:41.608484Z","shell.execute_reply.started":"2024-06-11T17:13:41.575054Z"},"id":"j04xaRci9wO_","trusted":true},"outputs":[],"source":["VOCAB_SIZE = 10000\n","SEQ_LEN = 28\n","\n","BATCH_SIZE = 256"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["1f118101669c4795ae8c0e56f1c7fa14","1030c057ea7f419a96081d8912cbbc73","8dd0ca5723ce40c094048f466ae71264","523ffa9dc6fb489e897e796cf402ffb9","e159b3ec68ff4aa2b57a12491fe5befe","ae5be16f0efa4ba7a9d31945bc4a8f27","6ee1e468ba7943889ef6d9e3d83df080","c8743d079e514edaaafc999360d8b677","fac53eeb7d74431396525900e4bfebbc","7a7e15aa62274a6da76de4e8d7fc8aa8","1ef747a4912d444d89e15bf048217203","e1f24264b0e14585ac9d536a5504ab3a","95195a7eb56e45de88984191b0430129","0e98f3b5915844fe9766bec29d507fa3","f32f292355c44feaac328b5d5b21d375","cd527f73adf547df8ee2e259d838a4b4","1ad2a42b9152498c9617225383ddc1a7","f1d89c301063459baf3390ffa6653d14","0f69f0a004d04ba7be6ee8f521784772","d871a4623c7247c187603ea32c4ed9f6","6f0dc3aa956b4d46834f27e4c77d5342","993d836e97d841a08de8cda83b97c784","361084c2af9b4f11be6384a96b66dd08","6ba6a23049d34011b868ba9d74712d97","c25aa7030a8d469ea1df97dc6c5c5c9e","932e0e28a9854f65a1ce47afe56b03a8","8f2bb2bc91d043dcbef5c60f8ab29748","f8e1666b04c240e68847f13dd71d1984","40a5e917c97e44e7b736387891d09a23","44ba2dff32cc4ab3954e9f00ab7c62c4","733e4e4840ad44298c77897a0f9c986e","f5698a85a2a94a359e7d2955ef540c90","af90885027614576b2c93b7cc44c74f7","3ac9ca9c7d244ae9b36872e52f355a5b","177fa266890940a987607b76f9d7103c","1684f55424c249f1bd366ae370445aae","5dff8da469ec45eeb6f37fae2b3e8813","2d3c9b153efc42c08618c739ef6c240e","012994bab4a3491dbd4c6abc073b9b42","2a8f11b5a155479a9e7b7ce3b8346185","a42ea5ac7da9425eb8c75f43fb3c7ca0","d49ea3900afd470baa0f877a0136b70e","4c107f062c4b4d2b8eab89c98f579f04","16e54011c3de4170ad0968b1433a5f3d","594f11e87a834d47a8075fbea4a30604","9f5ccd640b8f4631a88f99534b450797","dcdc83175b9a4e39b71f54d37a18c58d","04551dacf8de4bd0b27822c0aba8e649","e533c9aaee554b5ebb516bdaca8e9456","c577ccf270204598abec83bb751f642e","6b4c20e56a7146eba95a481fb6ea92d8","a3447f58ed0c416680c98afee5aae38e","32dc48a9e5564a62b0d3d98fef867236","4e202ad817b64903868c588473a09ff8","25a7cfcdabc742deb4c5b1bbc5aa80de"]},"execution":{"iopub.execute_input":"2024-06-11T17:13:41.611700Z","iopub.status.busy":"2024-06-11T17:13:41.611409Z","iopub.status.idle":"2024-06-11T17:15:27.797118Z","shell.execute_reply":"2024-06-11T17:15:27.796133Z","shell.execute_reply.started":"2024-06-11T17:13:41.611676Z"},"id":"sihcZOrp9wPB","outputId":"fef017d3-aa0b-48f6-f58d-529455634d1a","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"310d8cb3371c4971aff44a9192624342","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14983c3128a24422acc75b944ff5c2d5","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"117ef5fd824f491a88aacadc33969636","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/27.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6433f1b4cf24c2cb07cc2c5ac28496f","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1020868 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d354861e10df4ca38fd90321fe269a8a","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1020868 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ds = load_dataset('generics_kb', trust_remote_code=True)['train']\n","ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8)"]},{"cell_type":"markdown","metadata":{"id":"-57Vni-5cU99"},"source":["### Create the tokenizer and detokenizer\n","Define the tokens that are gonna be used by the tokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:27.798665Z","iopub.status.busy":"2024-06-11T17:15:27.798370Z","iopub.status.idle":"2024-06-11T17:15:27.803025Z","shell.execute_reply":"2024-06-11T17:15:27.802021Z","shell.execute_reply.started":"2024-06-11T17:15:27.798640Z"},"id":"xwsxyqqN9wPA","trusted":true},"outputs":[],"source":["#Define a class that contains all the token that we are gonna need\n","class Tokens:\n","    COMMA = '<comma>'\n","    START = '<start>'\n","    END = '<end>'"]},{"cell_type":"markdown","metadata":{"id":"EvnTgyALcdLh"},"source":["Add the tokens to the original sentences"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:27.804478Z","iopub.status.busy":"2024-06-11T17:15:27.804195Z","iopub.status.idle":"2024-06-11T17:15:33.275941Z","shell.execute_reply":"2024-06-11T17:15:33.275091Z","shell.execute_reply.started":"2024-06-11T17:15:27.804454Z"},"id":"kb5wPfO79wPB","trusted":true},"outputs":[],"source":["# Define a vectorized function to add the token to the oriinal string inside the dataset\n","add_token_vect = np.vectorize(\n","    lambda x: f'{Tokens.START} ' + x.replace(',', f' {Tokens.COMMA}') + f' {Tokens.END}')\n","\n","# Apply the function to the 'generic_sentence' column of the DataFrame\n","corpus = add_token_vect(ds['generic_sentence'])"]},{"cell_type":"markdown","metadata":{"id":"a7tmpEy0cs9J"},"source":["Create a custom preprocessing function in order to delete every special character from the sentences contained into the original dataset that are not the ones encoded into or part of tokens"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:33.277352Z","iopub.status.busy":"2024-06-11T17:15:33.277054Z","iopub.status.idle":"2024-06-11T17:15:33.283013Z","shell.execute_reply":"2024-06-11T17:15:33.282059Z","shell.execute_reply.started":"2024-06-11T17:15:33.277328Z"},"id":"0bQvBDJ79wPB","trusted":true},"outputs":[],"source":["#this function is gonna remove every special character that is not an `<>,` from the original sentences.\n","def custom_preprocessing(text):\n","    chars = string.punctuation\n","    chars = chars.replace(\",\", \"\")\n","    chars = chars.replace(\"<\", \"\")\n","    chars = chars.replace(\">\", \"\")\n","    # Remove punctuation\n","    text = tf.strings.regex_replace(text, '[%s]' % re.escape(chars), '')\n","    # Lowercase\n","    text = tf.strings.lower(text)\n","    # Remove punctuation\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"rG6t3_3vc7yD"},"source":["Create the tokenizer, using the `custom_preprocessing` function in order to standardise the input  "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-11T17:15:33.284347Z","iopub.status.busy":"2024-06-11T17:15:33.284056Z","iopub.status.idle":"2024-06-11T17:15:36.768018Z","shell.execute_reply":"2024-06-11T17:15:36.767045Z","shell.execute_reply.started":"2024-06-11T17:15:33.284317Z"},"id":"m4NaVqz69wPB","outputId":"107ff4a5-2c07-48f7-caee-fe015239f480","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['', '[UNK]', '<start>', '<end>', 'the', 'of', 'and', '<comma>', 'is', 'to']\n"]}],"source":["tokenizer = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    standardize=custom_preprocessing,\n","    output_sequence_length=SEQ_LEN,\n","    output_mode='int',\n","    pad_to_max_tokens=True,\n",")\n","\n","#adapt the tokenizer to the text of the ds\n","tokenizer.adapt(corpus)\n","\n","vocab = tokenizer.get_vocabulary()\n","\n","#visualize the first 10 tokens of the tokenizer\n","print(vocab[:10])"]},{"cell_type":"markdown","metadata":{"id":"gEmjfV-ed39I"},"source":["Create a detokenizer. This class is gonna be especially usefull during the test part, where we would need to detokenize the sentences to compare them with the original ones in order to calculate the score"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:36.769520Z","iopub.status.busy":"2024-06-11T17:15:36.769214Z","iopub.status.idle":"2024-06-11T17:15:40.539946Z","shell.execute_reply":"2024-06-11T17:15:40.539079Z","shell.execute_reply.started":"2024-06-11T17:15:36.769495Z"},"id":"9jyqE4529wPC","trusted":true},"outputs":[],"source":["class TextDetokenizer:\n","    def __init__(self, vectorize_layer):\n","        self.vectorize_layer = vectorize_layer\n","        vocab = self.vectorize_layer.get_vocabulary()\n","        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n","\n","    def __detokenize_tokens(self, tokens):\n","        def check_token(t):\n","            if t == 2:\n","                s = \"<start>\"\n","            elif t == 3:\n","                s = \"<end>\"\n","            elif t == 7:\n","                s = \"<comma>\"\n","            else:\n","                s = self.index_to_word.get(t, '[UNK]')\n","            return s\n","\n","        return ' '.join([check_token(token) for token in tokens if token != 0])\n","\n","    def __call__(self, batch_tokens):\n","        return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n","\n","#instantiate the detokenizer\n","detokenizer = TextDetokenizer(tokenizer)\n","\n","#tokenize the content of the whole dataset (corpus)\n","sentences = tokenizer( corpus ).numpy()"]},{"cell_type":"markdown","metadata":{"id":"2MZX-g6BeZN7"},"source":["Delete from the dataset all the sentences containing at least one `[UNK]` token"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-11T17:15:40.543378Z","iopub.status.busy":"2024-06-11T17:15:40.543095Z","iopub.status.idle":"2024-06-11T17:15:40.606115Z","shell.execute_reply":"2024-06-11T17:15:40.605187Z","shell.execute_reply.started":"2024-06-11T17:15:40.543355Z"},"id":"bQahAe8a9wPD","outputId":"6a2d6588-62e0-4c0d-b0b1-9b698cd6beef","trusted":true},"outputs":[{"data":{"text/plain":["(241194, 28)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["mask = np.sum( (sentences==1), axis=1) >= 1\n","original_data = np.delete( sentences, mask , axis=0)\n","original_data.shape"]},{"cell_type":"markdown","metadata":{"id":"oEDBf4rhcFLG"},"source":["### Create a generator to feed the data to the model\n","\n","Since we are gonna work with the transformer architecture we are gonna need to define a generator wich is gonna provide the appropriate input.\n","\n","The outpus is gonna be compose by 2 variable, a tuple (containing the input information) and the target variable:\n","\n","`encoder_input` : the input of the model (scrambled sentence)\n","\n","`decoder_input` : the previus knowledge of the model regarding to the next word to be generated. Similar to decoder_output but preceeded by the `<start>` token\n","\n","`decoder_output` : final sequence that should be generated by the decoder (sentence before being scrambled)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.607612Z","iopub.status.busy":"2024-06-11T17:15:40.607322Z","iopub.status.idle":"2024-06-11T17:15:40.617615Z","shell.execute_reply":"2024-06-11T17:15:40.616677Z","shell.execute_reply.started":"2024-06-11T17:15:40.607588Z"},"id":"1ZXLkWB6od0R","trusted":true},"outputs":[],"source":["class DataGenerator(keras.utils.PyDataset):\n","    def __init__(self, data, batch_size=32, shuffle=True, seed=42, **kwargs):\n","        super().__init__(**kwargs)\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.seed = seed\n","        self.indexes = np.arange(len(self.data))\n","\n","    def __len__(self):\n","        return math.ceil(len(self.data) / self.batch_size)\n","\n","    def __getitem__(self, index):\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        data_batch = np.array([self.data[k] for k in indexes])\n","        result = np.copy(data_batch)\n","\n","        # shuffle the phrases inside the tags\n","        for i in range(data_batch.shape[0]):\n","            np.random.shuffle(data_batch[i, 1:data_batch[i].argmin() - 1])\n","\n","        encoder_input = data_batch\n","        decoder_input = np.copy(result)\n","        decoder_output = np.copy(result)\n","        decoder_output = decoder_output[:, 1:]\n","\n","        #we need to add a column of zeroes at the end in order to match the sizes\n","        decoder_output = np.pad(decoder_output, [[0, 0], [0, 1]], mode='constant')\n","\n","        return (encoder_input, decoder_input), decoder_output"]},{"cell_type":"markdown","metadata":{"id":"fRTZyjSHi2xj"},"source":["Since the dataset contains huge part of coerent text near to each other is really important to shuffle the data, in order to sparse as much as possible all the possible arguments and word.\n","\n","To do so we are gonna do a triple shuffle of the data."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.619640Z","iopub.status.busy":"2024-06-11T17:15:40.618861Z","iopub.status.idle":"2024-06-11T17:15:40.740240Z","shell.execute_reply":"2024-06-11T17:15:40.739391Z","shell.execute_reply.started":"2024-06-11T17:15:40.619613Z"},"id":"fNo_Jy3N8zHS","trusted":true},"outputs":[],"source":["# Make a random permutation of training and test set\n","np.random.seed(42)\n","# Shuffle the all data\n","# TO have a really effective shuffoling we shuffle multiple times\n","shuffled_indices = np.random.permutation(len(original_data))\n","shuffled_data = original_data[shuffled_indices]\n","shuffled_indices = np.random.permutation(len(shuffled_data))\n","shuffled_data = shuffled_data[shuffled_indices]\n","shuffled_indices = np.random.permutation(len(shuffled_data))\n","shuffled_data = shuffled_data[shuffled_indices]"]},{"cell_type":"markdown","metadata":{"id":"387VUrEzjT5X"},"source":["Create the train, validation and test generator from the cleaned and shuffoled dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.741699Z","iopub.status.busy":"2024-06-11T17:15:40.741396Z","iopub.status.idle":"2024-06-11T17:15:40.747325Z","shell.execute_reply":"2024-06-11T17:15:40.746096Z","shell.execute_reply.started":"2024-06-11T17:15:40.741673Z"},"id":"0-oFYQZI9wPE","trusted":true},"outputs":[],"source":["#split the dataset\n","train_generator = DataGenerator(shuffled_data[:220000], batch_size=BATCH_SIZE)\n","val_generator = DataGenerator(shuffled_data[220000:235000], batch_size=BATCH_SIZE)\n","test_generator = DataGenerator(shuffled_data[235000:], batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"TPOtocSrjdR_"},"source":["Following is reported an exemple of what the ouput of the generator looks like"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-11T17:15:40.748919Z","iopub.status.busy":"2024-06-11T17:15:40.748545Z","iopub.status.idle":"2024-06-11T17:15:40.777414Z","shell.execute_reply":"2024-06-11T17:15:40.776471Z","shell.execute_reply.started":"2024-06-11T17:15:40.748892Z"},"id":"oyTcnf0VSuu3","outputId":"38185378-3fe6-45c4-9974-5a8c0a532841","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["encoder_input:  <start> factor in of persons age a developing important disease plays an the chances <end>\n","dencoder_input:  <start> age plays an important factor in a persons chances of developing the disease <end>\n","target:  age plays an important factor in a persons chances of developing the disease <end>\n","\n","\n","encoder_input:  <start> exercise ease help also mental regular can stress related disturbances and <end>\n","dencoder_input:  <start> regular exercise can also help ease stress and related mental disturbances <end>\n","target:  regular exercise can also help ease stress and related mental disturbances <end>\n","\n","\n"]}],"source":["x, y = train_generator.__getitem__(1)\n","\n","x_encoder_inputs_detokenized = detokenizer(x[0])\n","x_decoder_inputs_detokenized = detokenizer(x[1])\n","y_decoded = detokenizer(y)\n","\n","for i in range(2):\n","\n","  print(\"encoder_input: \",(x_encoder_inputs_detokenized[i]))\n","  print(\"dencoder_input: \",(x_decoder_inputs_detokenized[i]))\n","  print(\"target: \", y_decoded[i])\n","\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Fo8MazCGBTv3"},"source":["# Metrics"]},{"cell_type":"markdown","metadata":{"id":"G0NOkuO0CfPo"},"source":["Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n","\n","1.  look for the longest substring w between s and p\n","2.  compute |w|/max(|s|,|p|)\n","\n","If the match is exact, the score is 1.\n","\n","When computing the score, you should NOT consider the start and end tokens.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a-aUrdlXDdVf"},"source":["The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.779085Z","iopub.status.busy":"2024-06-11T17:15:40.778677Z","iopub.status.idle":"2024-06-11T17:15:40.784359Z","shell.execute_reply":"2024-06-11T17:15:40.783433Z","shell.execute_reply.started":"2024-06-11T17:15:40.779052Z"},"id":"ulpTRdrF_huh","trusted":true},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","\n","def score(s, p):\n","    match = SequenceMatcher(None, s, p).find_longest_match()\n","    # print(match.size)\n","    return (match.size/max(len(p), len(s)))"]},{"cell_type":"markdown","metadata":{"id":"RB2YfjXNExM-"},"source":["Let's do an example."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-11T17:15:40.786018Z","iopub.status.busy":"2024-06-11T17:15:40.785676Z","iopub.status.idle":"2024-06-11T17:15:40.794530Z","shell.execute_reply":"2024-06-11T17:15:40.793684Z","shell.execute_reply.started":"2024-06-11T17:15:40.785987Z"},"id":"h17C8bVjEwur","outputId":"4f7dccf7-4bd7-43ec-e63c-8a1e92cb28e4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["your score is  0.5423728813559322\n"]}],"source":["original = \"at first henry wanted to be friends with the king of france\"\n","generated = \"henry wanted to be friends with king of france at the first\"\n","\n","print(\"your score is \", score(original, generated))"]},{"cell_type":"markdown","metadata":{"id":"BET8GqBvFugR"},"source":["The score must be computed as an average of at least 3K random examples taken form the test set."]},{"cell_type":"markdown","metadata":{"id":"4fwo7xj4GBW1"},"source":["# What to deliver"]},{"cell_type":"markdown","metadata":{"id":"i6uITuxOGHfJ"},"source":["You are supposed to deliver a single notebook, suitably commented.\n","The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n","\n","The notebook should contain a full trace of the training.\n","Weights should be made available on request.\n","\n","You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n","\n","# Good work!"]},{"cell_type":"markdown","metadata":{"id":"u4CSZD_SAcHm"},"source":["# Proposed model: seq to seq transformer\n","\n","The proposel model is a seq to seq transformer.\n","\n","This kind of model si composed by 2 main parts:\n","\n","- Encoder: read the input sequence (in this case the shuffled words) and produces a fixed-dimensional vector representation.\n","- Decoder: generate the output sequence (original sentence) from the input given by the Encoder.\n","\n","This kind of models are well known and largely used in natural language tasks (NLP) as may be translations, summarization and classifications.\n","\n","## Why the transformer\n","The main reason to choose this kind of architecture is the self-attention mechanism. This characteristic should help the model capture the semantic meaning of the words, helping it achive good performance in reorder the words inside a phrase\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Syxo_zfnCgGX"},"source":["## Define the transformer model\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.796175Z","iopub.status.busy":"2024-06-11T17:15:40.795885Z","iopub.status.idle":"2024-06-11T17:15:40.822965Z","shell.execute_reply":"2024-06-11T17:15:40.821962Z","shell.execute_reply.started":"2024-06-11T17:15:40.796151Z"},"id":"9cev-dXcBZkg","trusted":true},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n","        else:\n","            padding_mask = None\n","\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"dense_dim\": self.dense_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = ops.shape(inputs)[-1]\n","        positions = ops.arange(0, length, 1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if mask is None:\n","            return None\n","        else:\n","            return ops.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"sequence_length\": self.sequence_length,\n","                \"vocab_size\": self.vocab_size,\n","                \"embed_dim\": self.embed_dim,\n","            }\n","        )\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n","            padding_mask = ops.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = None\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = ops.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = ops.arange(sequence_length)[:, None]\n","        j = ops.arange(sequence_length)\n","        mask = ops.cast(i >= j, dtype=\"int32\")\n","        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = ops.concatenate(\n","            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n","            axis=0,\n","        )\n","        return ops.tile(mask, mult)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"latent_dim\": self.latent_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"1d976N0jCqmt"},"source":["Define a function for an easy construction of the model.\n","\n","Notice that we are not using the PositionalEmbedding in the encoder since there is no meaningfull structure in the input scrambled sentences"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.824605Z","iopub.status.busy":"2024-06-11T17:15:40.824234Z","iopub.status.idle":"2024-06-11T17:15:40.837506Z","shell.execute_reply":"2024-06-11T17:15:40.836614Z","shell.execute_reply.started":"2024-06-11T17:15:40.824573Z"},"id":"doNw85ntDq2s","trusted":true},"outputs":[],"source":["def get_model(embed_dim, latent_dim, num_heads, sequence_length, vocab_size):\n","  encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","  x = Embedding(vocab_size, embed_dim)(encoder_inputs)\n","  encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","  encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","  decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","  encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","  x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","  x = layers.Dropout(0.5)(x)\n","  decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","  decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","  decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","\n","  transformer = keras.Model(\n","      [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n","  )\n","\n","  return transformer"]},{"cell_type":"markdown","metadata":{"id":"tlGUPebbC0Io"},"source":["# Training the model\n","\n","## Custom Loss and Scheduling functions\n","\n","To help the model achive the best performance possible some custom function has been defined.\n","\n","- `custom_loss`: compute te loss starting by the one produced by the `SparseCategoricalCrossentropy` loss function, and then adding a penalty for each words of the decoded sentence out of place\n","\n","- `custom_accuracy`: use the same principe as the `custom_loss` function, but instead calculate how many words are in place inside the decoded sentence\n","\n","- `WarmupScheduler`: it's used to customize the learning rate during the training. This custom  scheduler adjusts the learning rate during training, gradually increasing it during the warm-up phase and then following a specific schedule based on the step count.  "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:15:40.839020Z","iopub.status.busy":"2024-06-11T17:15:40.838674Z","iopub.status.idle":"2024-06-11T17:15:40.851236Z","shell.execute_reply":"2024-06-11T17:15:40.850288Z","shell.execute_reply.started":"2024-06-11T17:15:40.838993Z"},"id":"GzJFIyKnamPX","trusted":true},"outputs":[],"source":["#instantiate a loss function used for the custom loss\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","#implements the mask loss\n","def custom_loss(label, pred):\n","    #instantiate a boolan mask with all true from the token <start> to <end>\n","    mask = label != 0\n","\n","    #calculate the loss\n","    loss = loss_object(label, pred)\n","\n","    #same dtype cast\n","    mask = tf.cast(mask, dtype=loss.dtype)\n","\n","    #fix to zero all the loss after the <end> token\n","    loss *= mask\n","\n","    #return the average of the loss\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","#same principle of the custom loss\n","def custom_accuracy(label, pred):\n","    pred = tf.argmax(pred, axis=2)\n","    label = tf.cast(label, pred.dtype)\n","\n","    # check where pred and labels are equals\n","    match = tf.math.equal(label, pred)\n","\n","    #mask where the label is != from zero (from end token to the end)\n","    mask = tf.math.logical_not(tf.math.equal(label, 0))\n","\n","    #see where the label is equal to prediction and both are not zeroes\n","    match = match & mask\n","\n","    #cast to dtype\n","    match = tf.cast(match, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","\n","    #return the average accuracy\n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)"]},{"cell_type":"markdown","metadata":{"id":"i5dGaCGzGbX2"},"source":["\n","## Define the model hyperparameters and training the model\n","\n","This are the parameters proposed for the final model:\n","- `EMBEDDING_DIM` = 128\n","- `LATENT_DIM` = 512\n","- `NUM_HEADS` = 20\n","\n","For a **total of ~8M parameters** for the final model, really small considering that is based on a transformer architecture\n","\n","\\\\\n","For the training we are choosing the following hyperparameters:\n","- `EPOCH` = 30\n","- `BATCH_SIZE` = 256\n","\n","\\\\\n","During the fit procedure we are also using 2 callbacks:\n","- `EarlyStopping`: monitors a specified validation metric (`val_loss`) during training and stops training early if the metric stops improving, preventing overfitting.\n","- `ModelCheckpoint`:  It is used to save the model's weights during training.\n","- `ReduceLROnPlateau`: Decrease the learning rate when the loss stops decreasing form more than 2 epochs."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-06-11T17:15:40.852659Z","iopub.status.busy":"2024-06-11T17:15:40.852368Z","iopub.status.idle":"2024-06-11T17:47:24.669845Z","shell.execute_reply":"2024-06-11T17:47:24.669082Z","shell.execute_reply.started":"2024-06-11T17:15:40.852631Z"},"id":"637aCsQ2DtE5","outputId":"f0a1b417-1ac8-41a0-f8eb-3de0fbf97b93","trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,450,752</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ functional_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,343,120</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │ transformer_enco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,280,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,450,752\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ functional_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m5,343,120\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m10000\u001b[0m)            │            │ transformer_enco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,073,872</span> (30.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,073,872\u001b[0m (30.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,073,872</span> (30.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,073,872\u001b[0m (30.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:602: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m  2/860\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 76ms/step - custom_accuracy: 0.0077 - loss: 9.1810      "]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1718126156.905991     118 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1718126156.935909     118 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - custom_accuracy: 0.2607 - loss: 5.5985\n","Epoch 1: val_loss improved from inf to 2.33572, saving model to final_model.weights.h5\n"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1718126226.524823     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: custom_accuracy,loss,val_custom_accuracy,val_loss\n","  current = self.get_monitor_value(logs)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 81ms/step - custom_accuracy: 0.2608 - loss: 5.5970 - val_custom_accuracy: 0.6309 - val_loss: 2.3357 - learning_rate: 0.0010\n","Epoch 2/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.6455 - loss: 2.1978\n","Epoch 2: val_loss improved from 2.33572 to 1.41016, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.6455 - loss: 2.1975 - val_custom_accuracy: 0.7289 - val_loss: 1.4102 - learning_rate: 0.0010\n","Epoch 3/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.7333 - loss: 1.4353\n","Epoch 3: val_loss improved from 1.41016 to 1.18767, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.7333 - loss: 1.4353 - val_custom_accuracy: 0.7592 - val_loss: 1.1877 - learning_rate: 0.0010\n","Epoch 4/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.7710 - loss: 1.1499\n","Epoch 4: val_loss improved from 1.18767 to 1.10106, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.7710 - loss: 1.1499 - val_custom_accuracy: 0.7697 - val_loss: 1.1011 - learning_rate: 0.0010\n","Epoch 5/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.7939 - loss: 0.9952\n","Epoch 5: val_loss improved from 1.10106 to 1.04519, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 71ms/step - custom_accuracy: 0.7939 - loss: 0.9952 - val_custom_accuracy: 0.7784 - val_loss: 1.0452 - learning_rate: 0.0010\n","Epoch 6/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8109 - loss: 0.8846\n","Epoch 6: val_loss improved from 1.04519 to 1.03110, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.8109 - loss: 0.8846 - val_custom_accuracy: 0.7860 - val_loss: 1.0311 - learning_rate: 0.0010\n","Epoch 7/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8243 - loss: 0.8018\n","Epoch 7: val_loss improved from 1.03110 to 1.02559, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.8243 - loss: 0.8018 - val_custom_accuracy: 0.7881 - val_loss: 1.0256 - learning_rate: 0.0010\n","Epoch 8/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8356 - loss: 0.7354\n","Epoch 8: val_loss did not improve from 1.02559\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.8356 - loss: 0.7355 - val_custom_accuracy: 0.7907 - val_loss: 1.0414 - learning_rate: 0.0010\n","Epoch 9/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8448 - loss: 0.6841\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","\n","Epoch 9: val_loss did not improve from 1.02559\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.8448 - loss: 0.6841 - val_custom_accuracy: 0.7932 - val_loss: 1.0611 - learning_rate: 0.0010\n","Epoch 10/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8759 - loss: 0.5228\n","Epoch 10: val_loss improved from 1.02559 to 1.01065, saving model to final_model.weights.h5\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - custom_accuracy: 0.8759 - loss: 0.5227 - val_custom_accuracy: 0.8066 - val_loss: 1.0106 - learning_rate: 1.0000e-04\n","Epoch 11/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8916 - loss: 0.4472\n","Epoch 11: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.8916 - loss: 0.4472 - val_custom_accuracy: 0.8078 - val_loss: 1.0352 - learning_rate: 1.0000e-04\n","Epoch 12/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.8984 - loss: 0.4137\n","Epoch 12: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","\n","Epoch 12: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.8984 - loss: 0.4137 - val_custom_accuracy: 0.8081 - val_loss: 1.0493 - learning_rate: 1.0000e-04\n","Epoch 13/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9052 - loss: 0.3831\n","Epoch 13: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9052 - loss: 0.3831 - val_custom_accuracy: 0.8087 - val_loss: 1.0591 - learning_rate: 1.0000e-05\n","Epoch 14/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9069 - loss: 0.3760\n","Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.\n","\n","Epoch 14: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9069 - loss: 0.3760 - val_custom_accuracy: 0.8087 - val_loss: 1.0618 - learning_rate: 1.0000e-05\n","Epoch 15/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9075 - loss: 0.3725\n","Epoch 15: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9075 - loss: 0.3725 - val_custom_accuracy: 0.8089 - val_loss: 1.0657 - learning_rate: 1.0000e-05\n","Epoch 16/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9084 - loss: 0.3699\n","Epoch 16: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9084 - loss: 0.3699 - val_custom_accuracy: 0.8087 - val_loss: 1.0708 - learning_rate: 1.0000e-05\n","Epoch 17/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9089 - loss: 0.3681\n","Epoch 17: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9089 - loss: 0.3681 - val_custom_accuracy: 0.8086 - val_loss: 1.0732 - learning_rate: 1.0000e-05\n","Epoch 18/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9097 - loss: 0.3627\n","Epoch 18: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9097 - loss: 0.3627 - val_custom_accuracy: 0.8084 - val_loss: 1.0758 - learning_rate: 1.0000e-05\n","Epoch 19/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9100 - loss: 0.3629\n","Epoch 19: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9100 - loss: 0.3629 - val_custom_accuracy: 0.8086 - val_loss: 1.0792 - learning_rate: 1.0000e-05\n","Epoch 20/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9106 - loss: 0.3589\n","Epoch 20: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9106 - loss: 0.3589 - val_custom_accuracy: 0.8085 - val_loss: 1.0843 - learning_rate: 1.0000e-05\n","Epoch 21/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9109 - loss: 0.3584\n","Epoch 21: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 70ms/step - custom_accuracy: 0.9109 - loss: 0.3584 - val_custom_accuracy: 0.8083 - val_loss: 1.0855 - learning_rate: 1.0000e-05\n","Epoch 22/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9118 - loss: 0.3547\n","Epoch 22: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9118 - loss: 0.3547 - val_custom_accuracy: 0.8084 - val_loss: 1.0899 - learning_rate: 1.0000e-05\n","Epoch 23/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9117 - loss: 0.3534\n","Epoch 23: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9117 - loss: 0.3534 - val_custom_accuracy: 0.8082 - val_loss: 1.0945 - learning_rate: 1.0000e-05\n","Epoch 24/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9129 - loss: 0.3500\n","Epoch 24: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9129 - loss: 0.3500 - val_custom_accuracy: 0.8084 - val_loss: 1.0961 - learning_rate: 1.0000e-05\n","Epoch 25/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9134 - loss: 0.3471\n","Epoch 25: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9134 - loss: 0.3471 - val_custom_accuracy: 0.8081 - val_loss: 1.0988 - learning_rate: 1.0000e-05\n","Epoch 26/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9135 - loss: 0.3466\n","Epoch 26: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - custom_accuracy: 0.9135 - loss: 0.3466 - val_custom_accuracy: 0.8082 - val_loss: 1.1006 - learning_rate: 1.0000e-05\n","Epoch 27/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - custom_accuracy: 0.9146 - loss: 0.3420\n","Epoch 27: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - custom_accuracy: 0.9146 - loss: 0.3420 - val_custom_accuracy: 0.8077 - val_loss: 1.1057 - learning_rate: 1.0000e-05\n","Epoch 28/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - custom_accuracy: 0.9144 - loss: 0.3425\n","Epoch 28: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - custom_accuracy: 0.9144 - loss: 0.3425 - val_custom_accuracy: 0.8077 - val_loss: 1.1088 - learning_rate: 1.0000e-05\n","Epoch 29/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - custom_accuracy: 0.9147 - loss: 0.3411\n","Epoch 29: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - custom_accuracy: 0.9147 - loss: 0.3411 - val_custom_accuracy: 0.8080 - val_loss: 1.1094 - learning_rate: 1.0000e-05\n","Epoch 30/30\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - custom_accuracy: 0.9152 - loss: 0.3389\n","Epoch 30: val_loss did not improve from 1.01065\n","\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 69ms/step - custom_accuracy: 0.9152 - loss: 0.3389 - val_custom_accuracy: 0.8077 - val_loss: 1.1138 - learning_rate: 1.0000e-05\n"]}],"source":["#Parameters of the models\n","EMBEDDING_DIM = 128\n","LATENT_DIM = 512\n","NUM_HEADS = 20\n","EPOCH = 30\n","\n","run_name = 'final_model'\n","\n","transformer = get_model(EMBEDDING_DIM, LATENT_DIM, NUM_HEADS, SEQ_LEN, VOCAB_SIZE)\n","transformer.summary()\n","\n","\n","optimizer = keras.optimizers.AdamW(learning_rate=1e-3)\n","transformer.compile(optimizer=optimizer, loss=custom_loss, metrics=[custom_accuracy])\n","\n","\n","history = transformer.fit(train_generator, epochs=EPOCH, validation_data=val_generator, callbacks =\n","                                  [\n","                                  tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode = \"max\", patience=3, restore_best_weights=True),\n","                                  tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.00001, verbose=1),\n","                                  tf.keras.callbacks.ModelCheckpoint(run_name + '.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n","                                  ])"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:47:24.672059Z","iopub.status.busy":"2024-06-11T17:47:24.671414Z","iopub.status.idle":"2024-06-11T17:47:24.975848Z","shell.execute_reply":"2024-06-11T17:47:24.974976Z","shell.execute_reply.started":"2024-06-11T17:47:24.672023Z"},"id":"nfunsTZlDvCg","trusted":true},"outputs":[],"source":["#save the best model weights\n","weights_file_name = './' + run_name + '.weights.h5'\n","transformer.save_weights(weights_file_name)"]},{"cell_type":"markdown","metadata":{"id":"u2Z9Rpj1JvMw"},"source":["# Testing the model\n","\n","In the following section we are going to evaluate the results of the trained model, basing the statistics on the given metric"]},{"cell_type":"markdown","metadata":{"id":"mRx3uTEkLPik"},"source":["## Reload the best model found\n","If `laod_weights` parameters is set to true it will upload the best weight foud so far. Needed in order to test the model without retraining it."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:47:24.977773Z","iopub.status.busy":"2024-06-11T17:47:24.977134Z","iopub.status.idle":"2024-06-11T17:47:24.982424Z","shell.execute_reply":"2024-06-11T17:47:24.981534Z","shell.execute_reply.started":"2024-06-11T17:47:24.977739Z"},"id":"zwvKMh4UamPY","trusted":true},"outputs":[],"source":["# Load the Drive helper and mount\n","laod_weights = False\n","\n","if laod_weights:\n","  # Loads the weights\n","  transformer.load_weights(\"./final_model.weights.h5\")"]},{"cell_type":"markdown","metadata":{"id":"iPrh0zP_KNc7"},"source":["### Define a function to decode an input sequence\n","\n","`decode_sequence()`: takes an input scrambled sentence. It uses the training transformer model to predict the next token in a sequence, gradually building a decoded sentence until an “END” token is encountered or the maximum length is reached.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:47:24.984029Z","iopub.status.busy":"2024-06-11T17:47:24.983657Z","iopub.status.idle":"2024-06-11T17:47:25.037846Z","shell.execute_reply":"2024-06-11T17:47:25.036990Z","shell.execute_reply.started":"2024-06-11T17:47:24.983997Z"},"id":"Jv9B20SwUmpr","trusted":true},"outputs":[],"source":["vocab = tokenizer.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(vocab)), vocab))\n","max_decoded_sentence_length = 27\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = tokenizer([input_sentence])\n","    decoded_sentence = Tokens.START\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = tokenizer([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = ops.convert_to_numpy(\n","            ops.argmax(predictions[0, i, :])\n","        ).item(0)\n","\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == Tokens.END:\n","            break\n","    return decoded_sentence"]},{"cell_type":"markdown","metadata":{"id":"DEAb3wwBL5H2"},"source":["## Define a testing function\n","\n","`transformer`: the model to evaluate \n","\n","`test_generator`: the test_generator \n","\n","`batch_size`:  the size of each batch of the test generator \n","\n","`n_test_elem`: on how phrase test the model on \n","\n","`verbouse`: if put to True then it will print all the log of the process, composed by the inpput, the model prediction and the calculated score"]},{"cell_type":"code","execution_count":24,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-11T17:47:25.039565Z","iopub.status.busy":"2024-06-11T17:47:25.039265Z","iopub.status.idle":"2024-06-11T17:47:25.052660Z","shell.execute_reply":"2024-06-11T17:47:25.051784Z","shell.execute_reply.started":"2024-06-11T17:47:25.039540Z"},"id":"Jqnsc3vYUnRi","trusted":true},"outputs":[],"source":["def test_model(transformer, test_generator, batch_size, n_test_elem, verbouse=False, return_model_predictions=True, log_status=True):\n","    \n","    num_batch_test= math.ceil(n_test_elem / batch_size)\n","    \n","    if log_status: \n","        print(\"testing on: \", num_batch_test, \" batches\")\n","    \n","    scores = np.array([])\n","    result_list = []\n","\n","    for n_batch in range(num_batch_test):\n","        \n","        print(f'starting analysing batch n. {n_batch+1} out of {num_batch_test}')\n","        x, y = test_generator.__getitem__(n_batch)\n","        x_encoder_inputs_detokenized = detokenizer(x[0])\n","        x_decoder_inputs_detokenized = detokenizer(x[1])\n","        y_decoded = detokenizer(y)\n","\n","        total_phrase_num = len(x[0])\n","        for i in range(total_phrase_num):\n","          \n","          if i%32 == 0 and log_status: \n","                print(f\"\\tstatus {i+1}/{total_phrase_num}\")\n","            \n","          input_sentence = (x_encoder_inputs_detokenized[i])\n","          prediction = decode_sequence(input_sentence)\n","\n","          #add prediction to result list. The result list is gonna be a list of dictionary\n","          target = y_decoded[i].replace(Tokens.START, \"\")\n","          target = target.replace(Tokens.END, \"\")\n","\n","          prediction = prediction.replace(Tokens.START, \"\")\n","          prediction = prediction.replace(Tokens.END, \"\")\n","          prediction = prediction.lstrip()\n","\n","          calc_score = score(target, prediction)\n","          scores = np.append(scores, calc_score)\n","    \n","\n","          if verbouse:\n","            print(f\"Batch {n_batch+1}/{num_batch_test}, phrase {i+1}/{total_phrase_num}\")\n","            print('original: ', target)\n","            print('input: ', input_sentence)\n","            print('prediction: ', prediction)\n","            print('score: ', calc_score)\n","            print('---'*4)\n","        \n","          if return_model_predictions:\n","                result_list.append({\n","                    'original': target, \n","                    'input' : input_sentence, \n","                    'prediction' : prediction, \n","                    'score' : calc_score\n","                })\n","        \n","        print('Batch end, scores so far: ')\n","        print(f\"\\tscore:\\t{scores.mean()}\")\n","        print(f\"\\tstd:\\t{scores.std()}\")\n","        print('--'*4)\n","\n","    #print the results \n","    scores = np.array(scores)\n","    print(f\"\\n\\nTested on {num_batch_test*batch_size} examples \\n\")\n","    print(f\"Resoults :\")\n","    print(f\"\\tscore:\\t{scores.mean()}\")\n","    print(f\"\\tstd:\\t{scores.std()}\")\n","    \n","    if return_model_predictions: \n","        return scores, result_list\n","    \n","    return scores"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T17:47:25.054107Z","iopub.status.busy":"2024-06-11T17:47:25.053775Z","iopub.status.idle":"2024-06-11T18:25:26.112555Z","shell.execute_reply":"2024-06-11T18:25:26.111628Z","shell.execute_reply.started":"2024-06-11T17:47:25.054079Z"},"id":"g7RyfmxpamPZ","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["starting analysing batch n. 1 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.5073161963063024\n","\tstd:\t0.2852046958451032\n","--------\n","starting analysing batch n. 2 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.5050732153126545\n","\tstd:\t0.2819123532885841\n","--------\n","starting analysing batch n. 3 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4962788556283621\n","\tstd:\t0.2831283523432325\n","--------\n","starting analysing batch n. 4 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.49347653758049237\n","\tstd:\t0.28095406924554883\n","--------\n","starting analysing batch n. 5 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4941662950599661\n","\tstd:\t0.28005937781620627\n","--------\n","starting analysing batch n. 6 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4898037438468042\n","\tstd:\t0.27911145391882236\n","--------\n","starting analysing batch n. 7 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4910350354198184\n","\tstd:\t0.27916190303512317\n","--------\n","starting analysing batch n. 8 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4898299427357385\n","\tstd:\t0.2792556177390844\n","--------\n","starting analysing batch n. 9 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.49061649377675765\n","\tstd:\t0.27948967184371826\n","--------\n","starting analysing batch n. 10 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4911811440460889\n","\tstd:\t0.2792841723035333\n","--------\n","starting analysing batch n. 11 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.4912046243527044\n","\tstd:\t0.2796165935236492\n","--------\n","starting analysing batch n. 12 out of 12\n","Batch end, scores so far: \n","\tscore:\t0.48965417685765344\n","\tstd:\t0.2785067596872517\n","--------\n","\n","\n","Tested on 3072 examples \n","\n","Resoults :\n","\tscore:\t0.48965417685765344\n","\tstd:\t0.2785067596872517\n"]}],"source":["scores, result_list = test_model(transformer, test_generator, batch_size=BATCH_SIZE, n_test_elem=3000, verbouse=False, log_status=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Results and conclusions\n","As we can see from the outcome of the evaluation function the proposed model reach a score of `~0.49` using the provided scoring function, way above the estimated performance of a random classifier, estimated to be ~0.19 with a standard deviation of ~0.06. "]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"012994bab4a3491dbd4c6abc073b9b42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04551dacf8de4bd0b27822c0aba8e649":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e202ad817b64903868c588473a09ff8","placeholder":"​","style":"IPY_MODEL_25a7cfcdabc742deb4c5b1bbc5aa80de","value":" 1020868/1020868 [00:12&lt;00:00, 91467.88 examples/s]"}},"0e98f3b5915844fe9766bec29d507fa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f69f0a004d04ba7be6ee8f521784772","max":11863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d871a4623c7247c187603ea32c4ed9f6","value":11863}},"0f69f0a004d04ba7be6ee8f521784772":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1030c057ea7f419a96081d8912cbbc73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae5be16f0efa4ba7a9d31945bc4a8f27","placeholder":"​","style":"IPY_MODEL_6ee1e468ba7943889ef6d9e3d83df080","value":"Downloading builder script: 100%"}},"1684f55424c249f1bd366ae370445aae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42ea5ac7da9425eb8c75f43fb3c7ca0","max":1020868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d49ea3900afd470baa0f877a0136b70e","value":1020868}},"16e54011c3de4170ad0968b1433a5f3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"177fa266890940a987607b76f9d7103c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_012994bab4a3491dbd4c6abc073b9b42","placeholder":"​","style":"IPY_MODEL_2a8f11b5a155479a9e7b7ce3b8346185","value":"Generating train split: 100%"}},"1ad2a42b9152498c9617225383ddc1a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ef747a4912d444d89e15bf048217203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f118101669c4795ae8c0e56f1c7fa14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1030c057ea7f419a96081d8912cbbc73","IPY_MODEL_8dd0ca5723ce40c094048f466ae71264","IPY_MODEL_523ffa9dc6fb489e897e796cf402ffb9"],"layout":"IPY_MODEL_e159b3ec68ff4aa2b57a12491fe5befe"}},"25a7cfcdabc742deb4c5b1bbc5aa80de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a8f11b5a155479a9e7b7ce3b8346185":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d3c9b153efc42c08618c739ef6c240e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32dc48a9e5564a62b0d3d98fef867236":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"361084c2af9b4f11be6384a96b66dd08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ba6a23049d34011b868ba9d74712d97","IPY_MODEL_c25aa7030a8d469ea1df97dc6c5c5c9e","IPY_MODEL_932e0e28a9854f65a1ce47afe56b03a8"],"layout":"IPY_MODEL_8f2bb2bc91d043dcbef5c60f8ab29748"}},"3ac9ca9c7d244ae9b36872e52f355a5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_177fa266890940a987607b76f9d7103c","IPY_MODEL_1684f55424c249f1bd366ae370445aae","IPY_MODEL_5dff8da469ec45eeb6f37fae2b3e8813"],"layout":"IPY_MODEL_2d3c9b153efc42c08618c739ef6c240e"}},"40a5e917c97e44e7b736387891d09a23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44ba2dff32cc4ab3954e9f00ab7c62c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c107f062c4b4d2b8eab89c98f579f04":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e202ad817b64903868c588473a09ff8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"523ffa9dc6fb489e897e796cf402ffb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a7e15aa62274a6da76de4e8d7fc8aa8","placeholder":"​","style":"IPY_MODEL_1ef747a4912d444d89e15bf048217203","value":" 8.64k/8.64k [00:00&lt;00:00, 330kB/s]"}},"594f11e87a834d47a8075fbea4a30604":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f5ccd640b8f4631a88f99534b450797","IPY_MODEL_dcdc83175b9a4e39b71f54d37a18c58d","IPY_MODEL_04551dacf8de4bd0b27822c0aba8e649"],"layout":"IPY_MODEL_e533c9aaee554b5ebb516bdaca8e9456"}},"5dff8da469ec45eeb6f37fae2b3e8813":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c107f062c4b4d2b8eab89c98f579f04","placeholder":"​","style":"IPY_MODEL_16e54011c3de4170ad0968b1433a5f3d","value":" 1020868/1020868 [01:45&lt;00:00, 13416.33 examples/s]"}},"6b4c20e56a7146eba95a481fb6ea92d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ba6a23049d34011b868ba9d74712d97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8e1666b04c240e68847f13dd71d1984","placeholder":"​","style":"IPY_MODEL_40a5e917c97e44e7b736387891d09a23","value":"Downloading data: 100%"}},"6ee1e468ba7943889ef6d9e3d83df080":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f0dc3aa956b4d46834f27e4c77d5342":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"733e4e4840ad44298c77897a0f9c986e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a7e15aa62274a6da76de4e8d7fc8aa8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dd0ca5723ce40c094048f466ae71264":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8743d079e514edaaafc999360d8b677","max":8643,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fac53eeb7d74431396525900e4bfebbc","value":8643}},"8f2bb2bc91d043dcbef5c60f8ab29748":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932e0e28a9854f65a1ce47afe56b03a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5698a85a2a94a359e7d2955ef540c90","placeholder":"​","style":"IPY_MODEL_af90885027614576b2c93b7cc44c74f7","value":" 27.1M/27.1M [00:01&lt;00:00, 22.9MB/s]"}},"95195a7eb56e45de88984191b0430129":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ad2a42b9152498c9617225383ddc1a7","placeholder":"​","style":"IPY_MODEL_f1d89c301063459baf3390ffa6653d14","value":"Downloading readme: 100%"}},"993d836e97d841a08de8cda83b97c784":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f5ccd640b8f4631a88f99534b450797":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c577ccf270204598abec83bb751f642e","placeholder":"​","style":"IPY_MODEL_6b4c20e56a7146eba95a481fb6ea92d8","value":"Filter: 100%"}},"a3447f58ed0c416680c98afee5aae38e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a42ea5ac7da9425eb8c75f43fb3c7ca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae5be16f0efa4ba7a9d31945bc4a8f27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af90885027614576b2c93b7cc44c74f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c25aa7030a8d469ea1df97dc6c5c5c9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44ba2dff32cc4ab3954e9f00ab7c62c4","max":27147920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_733e4e4840ad44298c77897a0f9c986e","value":27147920}},"c577ccf270204598abec83bb751f642e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8743d079e514edaaafc999360d8b677":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd527f73adf547df8ee2e259d838a4b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d49ea3900afd470baa0f877a0136b70e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d871a4623c7247c187603ea32c4ed9f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dcdc83175b9a4e39b71f54d37a18c58d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3447f58ed0c416680c98afee5aae38e","max":1020868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32dc48a9e5564a62b0d3d98fef867236","value":1020868}},"e159b3ec68ff4aa2b57a12491fe5befe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1f24264b0e14585ac9d536a5504ab3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95195a7eb56e45de88984191b0430129","IPY_MODEL_0e98f3b5915844fe9766bec29d507fa3","IPY_MODEL_f32f292355c44feaac328b5d5b21d375"],"layout":"IPY_MODEL_cd527f73adf547df8ee2e259d838a4b4"}},"e533c9aaee554b5ebb516bdaca8e9456":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d89c301063459baf3390ffa6653d14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f32f292355c44feaac328b5d5b21d375":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f0dc3aa956b4d46834f27e4c77d5342","placeholder":"​","style":"IPY_MODEL_993d836e97d841a08de8cda83b97c784","value":" 11.9k/11.9k [00:00&lt;00:00, 623kB/s]"}},"f5698a85a2a94a359e7d2955ef540c90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8e1666b04c240e68847f13dd71d1984":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac53eeb7d74431396525900e4bfebbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
